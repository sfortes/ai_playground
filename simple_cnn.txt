This C code is a from-scratch implementation of a simple Convolutional Neural Network (CNN) in C. I wrote it after conclusion of "Machine Learning Specialization" course from Andrew Ng for showing the fundamental mechanics of a CNN without the abstraction of high-level frameworks like TensorFlow or PyTorch.
The code defines the necessary data structures (matrices, layers) and functions to perform a full training cycle: forward propagation, loss calculation, backpropagation, and weight updates (using stochastic gradient descent). The network is hardcoded in the main function to solve a simple classification problem on dummy data.
While impressive as a pedagogical example, it lacks the performance, robustness, and flexibility of production-grade machine learning libraries due to its pure C implementation, manual memory management, and limited feature set.
I. Network Architecture and Data Structures
The network architecture is defined and instantiated in the main function. It consists of the following layers:
Convolutional Layer (Layer_Conv): 3 kernels (filters) of size 3x3.
ReLU Activation: Applied after the convolutional layer.
Max Pooling Layer (Layer_MaxPool): A 2x2 pooling window.
Flattening: (Implicitly handled by reshaping the matrix before the dense layer).
Dense (Fully Connected) Layer (Layer_Dense): Takes the flattened output from the pooling layer and maps it to 10 output neurons (for a 10-class classification problem).
Softmax Activation: Applied to the output of the dense layer to produce class probabilities.
Key Data Structures:
Matrix:
Generated c
typedef struct {
    int rows;
    int cols;
    double* data;
} Matrix;
Use code with caution.
C
This is the fundamental building block. It represents 2D arrays of data, which are used for images, feature maps, kernels, and weights. All operations are built on top of this structure.
Layer_Conv, Layer_Dense:
These structs hold the parameters (weights and biases) and their corresponding gradients for the convolutional and dense layers, respectively. They also store intermediate values needed for backpropagation (like the input to the layer).
CNN:
Generated c
typedef struct {
    Layer_Conv conv1;
    Layer_Dense dense1;
    // ... other parameters
} CNN;

C
This struct encapsulates the entire model, holding all the layers and parameters like the learning rate.
II. Analysis of Key ML Components
1. Forward Propagation
The code implements the forward pass, propagating an input image through the network layer by layer.
forward_conv(Matrix* input, Layer_Conv* layer):
This function performs the 2D convolution. It iterates through the input matrix with a sliding window (the kernel).
At each position, it calculates the dot product between the input patch and the kernel, adds the bias, and stores the result in the output feature map.
Limitations: The stride is hardcoded to 1, and there is no padding ('valid' convolution). This means the output feature map is smaller than the input.
forward_relu(Matrix* input):
Applies the Rectified Linear Unit function element-wise: output = max(0, input).
forward_maxpool(Matrix* input, Layer_MaxPool* layer):
Performs max pooling to downsample the feature maps. It slides a 2x2 window across the input and takes the maximum value.
Crucially, it stores the indices of the maximum values in layer->max_indices. This is essential for backpropagation.
forward_dense(Matrix* input, Layer_Dense* layer):
Performs the standard dense layer operation: output = (input * weights) + bias. This is implemented using the matrix_multiply and matrix_add_bias functions.
softmax(Matrix* logits):
Converts the final scores (logits) from the dense layer into a probability distribution.
It includes a common numerical stability trick: subtracting the maximum logit value from all logits before applying the exp() function to prevent overflow.
2. Loss Function
cross_entropy_loss(Matrix* probs, int true_label):
Calculates the categorical cross-entropy loss, which is standard for multi-class classification.
The formula used is -log(p_i), where p_i is the predicted probability for the correct class i. This is a simplified version for a single data point.
3. Backpropagation (The Core of Learning)
This is the most complex part of the code and demonstrates the underlying mathematics. The gradient of the loss is calculated and propagated backward through the network.
backward_dense(Layer_Dense* layer, Matrix* d_output):
d_output is the gradient from the subsequent layer (or the initial gradient from the loss function).
It calculates:
d_weights: Gradient for the weights, calculated as (input^T * d_output).
d_biases: Gradient for the biases, calculated by summing the columns of d_output.
d_input: Gradient to be passed to the previous layer, calculated as (d_output * weights^T).
This is the implementation of backpropagation for a dense layer.
backward_maxpool(Layer_MaxPool* layer, Matrix* d_output):
The gradient is only routed back to the neuron that had the maximum value during the forward pass. All other neurons in the pool get a gradient of 0.
It uses the max_indices saved during the forward pass to correctly place the gradient d_output into the larger d_input matrix.
d_relu(Matrix* input):
Calculates the derivative of ReLU element-wise. The derivative is 1 if input > 0 and 0 otherwise. The gradient from the next layer is then multiplied by this derivative (Hadamard product).
backward_conv(Layer_Conv* layer, Matrix* d_output):
This implements backpropagation through the convolutional layer.
d_kernels: The gradient for the kernels is calculated by convolving the layer's input with the output gradient (d_output).
d_biases: The gradient for the biases is the sum of all elements in the d_output for that feature map.
d_input: The gradient for the layer's input is the most complex part. It's calculated by performing a full convolution (convolution with padding) of the d_output with a 180-degree rotated version of the kernel. The code implements this with nested loops.
4. Parameter Update
cnn_update(CNN* cnn):
This function performs the Stochastic Gradient Descent (SGD) update rule:
weights = weights - learning_rate * d_weights
biases = biases - learning_rate * d_biases
It does this for both the convolutional and dense layers.
III. Strengths and Weaknesses
Strengths
Educational Value: This is the code's greatest strength. It demystifies what happens inside a CNN, showing every calculation explicitly. Anyone who understands this code has a solid grasp of the fundamentals.
Self-Contained: It has no external dependencies beyond the C standard library, making it highly portable and easy to compile.
Clear Structure: The use of structs and separate functions for forward/backward passes for each layer type is a good design pattern that mimics object-oriented principles.
Weaknesses
Performance: This implementation will be extremely slow for any non-trivial task.
No Vectorization: All operations are done with nested for loops in C. Modern CPUs have SIMD instructions (SSE, AVX) that can perform operations on multiple numbers at once. Libraries like BLAS/LAPACK are highly optimized to use these.
No GPU Acceleration: Modern ML is dominated by GPU parallelism (CUDA, OpenCL). This C code is single-threaded and CPU-bound.
Memory Management: It relies entirely on manual malloc and free.
This is highly error-prone. A single forgotten free could cause a memory leak, especially in the training loop where matrices are created and destroyed every step.
The cnn_free function helps, but intermediate matrices created during the forward/backward pass must be managed carefully.
Lack of Generality and Features:
No Batching: The network processes one sample at a time (Stochastic Gradient Descent). Modern training uses mini-batches for more stable gradients and better hardware utilization.
Hardcoded Architecture: The network structure is fixed in main. A flexible implementation would allow for dynamically adding/removing layers.
Limited Operations: It lacks common features like configurable stride/padding, different activation functions (e.g., LeakyReLU, Tanh), optimizers (e.g., Adam, RMSprop), or regularization (e.g., Dropout, L2).
Numerical Instability: While the softmax is protected, from-scratch implementations can be sensitive to vanishing/exploding gradients on deeper networks without techniques like careful weight initialization (here it's random) or batch normalization.
IV. Conclusion and Suggestions for Improvement
This code is an excellent academic exercise and a fantastic learning resource. It successfully implements the core logic of a CNN from the ground up.
If anybody wants to take this project further, here are some potential next steps:
Implement Mini-Batch Gradient Descent: Modify the training loop to process a batch of inputs at once. This would require adjusting the forward/backward functions to handle 3D/4D data tensors.
Generalize the Network Structure: Create a more dynamic CNN struct that holds an array of generic Layer pointers, allowing for the construction of arbitrary network architectures.
Add More Optimizers: Implement more advanced optimizers like Adam, which often converges faster and more reliably than SGD.
Integrate a BLAS library: For a significant performance boost, replace the manual matrix multiplication loops with calls to an optimized Basic Linear Algebra Subprograms (BLAS) library like OpenBLAS.
Improve Memory Management: Implement a simple arena/region allocator for the forward/backward pass. This would allocate a large block of memory at the start of a pass and "bump" a pointer for each new matrix, freeing the entire block at the end. This is faster and less error-prone than many individual malloc/free calls.
